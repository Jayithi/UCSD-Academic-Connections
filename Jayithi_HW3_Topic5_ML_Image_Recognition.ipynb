{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jayithi Gavva - IMAGE RECOGNITION-ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This has been quite a good resource for me. It helped me do my final project and also just really made me understand the base concept of CNN's, I haven't really added many notes in here. I have also used some of the comments in my final project to explain what I have done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "<br></br>\n",
    "Take me to the [code and Jupyter Notebook](https://github.com/AMoazeni/Machine-Learning-Image-Recognition/blob/master/Jupyter%20Notebook/ML%20-%20Image%20Recognition.ipynb) for Image Recognition!\n",
    "\n",
    "<br></br>\n",
    "This article explores a Machine Learning algorithm called Convolution Neural Network (CNN), it's a common Deep Learning technique used for image recognition and classification.\n",
    "\n",
    "<br></br>\n",
    "<div align=\"center\">\n",
    "<img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Data/single_prediction/cat_or_dog_1.jpg\" width=20% alt=\"Dog\">\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Data/single_prediction/cat_or_dog_2.jpg\" width=20% alt=\"Cat\">\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br></br>\n",
    "You are provided with a dataset consisting of 5,000 Cat images and 5,000 Dog images. We are going to train a Machine Learning model to learn differences between the two categories. The model will predict if a new unseen image is a Cat or Dog. The code architecture is robust and can be used to recognize any number of image categories, if provided with enough data.\n",
    "\n",
    "\n",
    "\n",
    "<br></br>\n",
    "\n",
    "# Convolution Neural Networks (CNN)\n",
    "\n",
    "<br></br>\n",
    "Convolution Neural Networks are good for pattern recognition and feature detection which is especially useful in image classification. Improve the performance of Convolution Neural Networks through hyper-parameter tuning, adding more convolution layers, adding more fully connected layers, or providing more correctly labeled data to the algorithm.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "Create a Convolution Neural Network (CNN) with the following steps:\n",
    "\n",
    "1. Convolution\n",
    "2. Max Pooling\n",
    "3. Flattening\n",
    "4. Full Connection\n",
    "\n",
    "\n",
    "<br></br>\n",
    "Check out [How to implement a neural network](http://peterroelants.github.io/posts/neural_network_implementation_intermezzo02/), also take a look at [A Friendly Introduction to Cross-Entropy Loss](http://rdipietro.github.io/friendly-intro-to-cross-entropy-loss/).\n",
    "\n",
    "\n",
    "<br></br>\n",
    "Convolution is a function derived from two other functions through an integration that expresses how the shape of one is modified by the other.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Jupyter%20Notebook/Images/01%20-%20Convolution%20Equation.png\" alt=\"Convolution-Equation\"></div>\n",
    "\n",
    "\n",
    "<br></br>\n",
    "For image recognition, we convolve the input image with Feature Detectors (also known as Kernel or Filter) to generate a Feature Map (also known as Convolved Map or Activation Map). This reveals and preserves patterns in the image, and also compresses the image for easier processing. Feature Maps are generated by element-wise multiplication and addition of corresponding images with Filters consisting of multiple Feature Detectors. This allows the creation of multiple Feature Maps.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Jupyter%20Notebook/Images/02%20-%20CNN%20Example.png\" width=\"500\" alt=\"CNN-Example\"></div>\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Jupyter%20Notebook/Images/03%20-%20CNN%20Feature%20Map.png\" width=\"500\" alt=\"CNN-Feature\"></div>\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Jupyter%20Notebook/Images/04%20-%20CNN%20Multi%20Feature%20Map.png\" width=\"500\" alt=\"Feature-Map\"></div>\n",
    "\n",
    "\n",
    "<br></br>\n",
    "This [Image Convolution Guide](https://docs.gimp.org/en/plug-in-convmatrix.html) allows you to play with various filters applied to an image. Edge Detect is a useful filters in Machine Learning. The algorithm creates filters that are not recognizable to humans, perhaps we learn with similar techniques in our subconscious. Feature Maps preserve spatial relationships between pixels throughout processing.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Jupyter%20Notebook/Images/05%20-%20Edge%20Detect%20Filter.png\" width=\"500\" alt=\"Edge-Detect\"></div>\n",
    "\n",
    "\n",
    "\n",
    "<br></br>\n",
    "\n",
    "# Rectified Linear Units (ReLU)\n",
    "\n",
    "<br></br>\n",
    "Rectifier Functions are applied to Convolution Neural Networks to increases non-linearity (breaks up linearity). This is an important step for image recognition with CNNs. Images are usually non-linear due to sharp transition of pixels, different colors, etc. ReLU functions help amplify the non-linearity of images so the ML model has an easier time finding patterns. \n",
    "\n",
    "<br></br>\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Jupyter%20Notebook/Images/06%20-%20ReLU%20Layer.png\" alt=\"ReLU\"></div>\n",
    "\n",
    "\n",
    "<br></br>\n",
    "### Before ReLU\n",
    "\n",
    "<br></br>\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Jupyter%20Notebook/Images/07%20-%20Before%20ReLU.png\" alt=\"Before-ReLU\"></div>\n",
    "\n",
    "\n",
    "<br></br>\n",
    "###  After ReLU\n",
    "\n",
    "<br></br>\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Jupyter%20Notebook/Images/08%20-%20After%20ReLU.png\" alt=\"After-ReLU\"></div>\n",
    "\n",
    "\n",
    "<br></br>\n",
    "In the above example, the ReLU operation removed the Black Pixels so there's less White to Gray to Black transitions. Borders now have more abrupt Pixel changes. Microsoft argues that the using their Modified Rectifier Function works better for CNNs.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Jupyter%20Notebook/Images/09%20-%20Modified%20Rectifier.png\" alt=\"Rectifier\"></div>\n",
    "\n",
    "\n",
    "\n",
    "<br></br>\n",
    "\n",
    "# Max Pooling\n",
    "\n",
    "<br></br>\n",
    "Max Pooling finds the largest value of small grids in the Feature Map, this creates a Pooled Feature Map. Average Pooling (sub-sampling) takes the average values of small grids.  It makes sure that your Neural Network has Spatial Invariance (able to find learned features in new images that are slightly varied or distorted). Max Pooling provides resilience against shifter or rotated features. It also further distills Feature Maps (reduces size) while preserving spatial relationships of pixels. Removing unnecessary information also helps prevent overfitting. Read 'Evaluation of Pooling Operations in Convolutional Architectures for Object Recognition.pdf'. Here is an online [CNN Visualization Tool](http://scs.ryerson.ca/~aharley/vis/conv/flat.html).\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Jupyter%20Notebook/Images/10%20-%20Max%20Pooling.png\" alt=\"Pooling\"></div>\n",
    "\n",
    "\n",
    "\n",
    "<br></br>\n",
    "\n",
    "# Flattening\n",
    "\n",
    "<br></br>\n",
    "Flattening puts values of the pooled Feature Map matrix into a 1-D vector. This makes it easy for the image data to pass through an Artificial Neural Network algorithm.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Jupyter%20Notebook/Images/11%20-%20Flattening.png\" width=\"400\" alt=\"Flattening\"></div>\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Jupyter%20Notebook/Images/12%20-%20Flattening%202.png\" width=\"400\" alt=\"Flattening-2\"></div>\n",
    "\n",
    "\n",
    "\n",
    "<br></br>\n",
    "\n",
    "# Full Connection\n",
    "\n",
    "<br></br>\n",
    "This is when the output of a Convolution Neural Network is flattened and fed through a classic Artificial Neural Network. It's important to note that CNNs require fully-connected hidden layers where as regular ANNs don't necessarily need full connections.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Jupyter%20Notebook/Images/13%20-%20Full%20Connection.png\" width=\"400\" alt=\"Full-Connection\"></div>\n",
    "\n",
    "\n",
    "<br></br>\n",
    "The process of CNN back-propagation adjusts weights of neurons, while adjusting Feature Maps.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Jupyter%20Notebook/Images/14%20-%20CNN%20Backprop.png\" width=\"400\" alt=\"Back-Propagation\"></div>\n",
    "\n",
    "\n",
    "<br></br>\n",
    "When it's time for the CNN to make a decision between Cat or Dog, the final layer neurons 'vote' on probability of an image being a Cat or Dog (or any other categories you show it). The Neural Network adjusts votes according to the best weights it has determined through back-propagation.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Jupyter%20Notebook/Images/15%20-%20CNN%20Weighted%20Votes.png\" alt=\"Weighted-Votes\"></div>\n",
    "\n",
    "\n",
    "<br></br>\n",
    "Here is a summary of every step of a CNN, don't forget about the Rectifier Function that removes linearity in Feature Maps, also remember that the hidden layers are fully connected.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Jupyter%20Notebook/Images/16%20-%20CNN%20Full.png\" alt=\"CNN-Full\"></div>\n",
    "\n",
    "\n",
    "\n",
    "<br></br>\n",
    "\n",
    "# Pre-Processing (Images Augmentation)\n",
    "\n",
    "<br></br>\n",
    "This step modifies images to prevent over-fitting. This data augmentation trick can generate tons more data by applying random modifications to existing data like shearing, stretching, zooming, etc. This makes your dataset and algorithm more robust and generalized.\n",
    "\n",
    "\n",
    "\n",
    "<br></br>\n",
    "\n",
    "# Softmax and Cross-Entropy Cost Function\n",
    "\n",
    "<br></br>\n",
    "The Softmax function shown below is used to make sure that the probabilities of the output layer add up to one, this gives us a percentage guess. Watch this Geoffrey Hinton [video about the SoftMax Function](https://www.youtube.com/watch?v=mlaLLQofmR8).\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Jupyter%20Notebook/Images/18%20-%20Softmax%20Function.png\" alt=\"SoftMax\"></div>\n",
    "\n",
    "\n",
    "<br></br>\n",
    "We had previously used the Mean Squared Error (MSE) Cost Function. For CNNs, it's better to use the Cross-Entropy Function as your Cost Function. We use Cross-Entropy as a Loss Function because it has a 'Log' term which helps amplify small Errors and better guide gradient descent.\n",
    "\n",
    "<br></br>\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Jupyter%20Notebook/Images/17%20-%20Log%20Loss%20Function.png\" alt=\"Loss-Function\"></div>\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Jupyter%20Notebook/Images/19%20-%20Cross%20Entropy%20Function.png\" width=\"200\" alt=\"Cross-Entropy\"></div>\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Jupyter%20Notebook/Images/20%20-%20Cross%20Entropy%20Plug%20In.png\" width=\"400\" alt=\"Cross-Entropy-2\"></div>\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Jupyter%20Notebook/Images/21%20-%20Error%20Comparison.png\" alt=\"Error\"></div>\n",
    "\n",
    "\n",
    "\n",
    "<br></br>\n",
    "\n",
    "# Code\n",
    "\n",
    "<br></br>\n",
    "Download the code and run it with 'Jupyter Notebook' or copy the code into the 'Spyder' IDE found in the [Anaconda Distribution](https://www.anaconda.com/download/). 'Spyder' is similar to MATLAB, it allows you to step through the code and examine the 'Variable Explorer' to see exactly how the data is parsed and analyzed. Jupyter Notebook also offers a [Jupyter Variable Explorer Extension](http://volderette.de/jupyter-notebook-variable-explorer/) which is quite useful for keeping track of variables.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "```shell\n",
    "$ git clone https://github.com/AMoazeni/Machine-Learning-Image-Recognition.git\n",
    "$ cd Machine-Learning-Image-Recognition\n",
    "```\n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tanish Reddy\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '../Data/training_set'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 76\u001b[0m\n\u001b[0;32m     71\u001b[0m test_datagen \u001b[38;5;241m=\u001b[39m ImageDataGenerator(rescale \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# 'batch_size' is the number of images that go through the CNN every weight update cycle\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Increase 'target_size' to improve model accuracy \u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m training_set \u001b[38;5;241m=\u001b[39m train_datagen\u001b[38;5;241m.\u001b[39mflow_from_directory(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../Data/training_set\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     77\u001b[0m                                                  target_size \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m),\n\u001b[0;32m     78\u001b[0m                                                  batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m,\n\u001b[0;32m     79\u001b[0m                                                  class_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     82\u001b[0m test_set \u001b[38;5;241m=\u001b[39m test_datagen\u001b[38;5;241m.\u001b[39mflow_from_directory(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../Data/test_set\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     83\u001b[0m                                             target_size \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m),\n\u001b[0;32m     84\u001b[0m                                             batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m,\n\u001b[0;32m     85\u001b[0m                                             class_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Tanish Reddy\\anaconda3\\Lib\\site-packages\\keras\\src\\legacy\\preprocessing\\image.py:1138\u001b[0m, in \u001b[0;36mImageDataGenerator.flow_from_directory\u001b[1;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[0;32m   1120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflow_from_directory\u001b[39m(\n\u001b[0;32m   1121\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1122\u001b[0m     directory,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1136\u001b[0m     keep_aspect_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1137\u001b[0m ):\n\u001b[1;32m-> 1138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DirectoryIterator(\n\u001b[0;32m   1139\u001b[0m         directory,\n\u001b[0;32m   1140\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1141\u001b[0m         target_size\u001b[38;5;241m=\u001b[39mtarget_size,\n\u001b[0;32m   1142\u001b[0m         color_mode\u001b[38;5;241m=\u001b[39mcolor_mode,\n\u001b[0;32m   1143\u001b[0m         keep_aspect_ratio\u001b[38;5;241m=\u001b[39mkeep_aspect_ratio,\n\u001b[0;32m   1144\u001b[0m         classes\u001b[38;5;241m=\u001b[39mclasses,\n\u001b[0;32m   1145\u001b[0m         class_mode\u001b[38;5;241m=\u001b[39mclass_mode,\n\u001b[0;32m   1146\u001b[0m         data_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_format,\n\u001b[0;32m   1147\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1148\u001b[0m         shuffle\u001b[38;5;241m=\u001b[39mshuffle,\n\u001b[0;32m   1149\u001b[0m         seed\u001b[38;5;241m=\u001b[39mseed,\n\u001b[0;32m   1150\u001b[0m         save_to_dir\u001b[38;5;241m=\u001b[39msave_to_dir,\n\u001b[0;32m   1151\u001b[0m         save_prefix\u001b[38;5;241m=\u001b[39msave_prefix,\n\u001b[0;32m   1152\u001b[0m         save_format\u001b[38;5;241m=\u001b[39msave_format,\n\u001b[0;32m   1153\u001b[0m         follow_links\u001b[38;5;241m=\u001b[39mfollow_links,\n\u001b[0;32m   1154\u001b[0m         subset\u001b[38;5;241m=\u001b[39msubset,\n\u001b[0;32m   1155\u001b[0m         interpolation\u001b[38;5;241m=\u001b[39minterpolation,\n\u001b[0;32m   1156\u001b[0m         dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[0;32m   1157\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Tanish Reddy\\anaconda3\\Lib\\site-packages\\keras\\src\\legacy\\preprocessing\\image.py:453\u001b[0m, in \u001b[0;36mDirectoryIterator.__init__\u001b[1;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio, dtype)\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[0;32m    452\u001b[0m     classes \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 453\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m subdir \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(directory)):\n\u001b[0;32m    454\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, subdir)):\n\u001b[0;32m    455\u001b[0m             classes\u001b[38;5;241m.\u001b[39mappend(subdir)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '../Data/training_set'"
     ]
    }
   ],
   "source": [
    "# Convolution Neural Network\n",
    "# Part 1 - Building CNN Architecture and Import Data\n",
    "\n",
    "# Importing the Keras libraries and packages\n",
    "# 'Sequential' library used to Initialize NN as sequence of layers (Alternative to Graph initialization)\n",
    "from tensorflow.keras.models import Sequential\n",
    "# 'Conv2D' for 1st step of adding convolution layers to images ('Conv3D' for videos with time as 3rd dimension)\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "# 'MaxPooling2D' step 2 for pooling of max values from Convolution Layers\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "# 'Flatten' Pooled Layers for step 3\n",
    "from tensorflow.keras.layers import Flatten\n",
    "# 'Dense' for fully connected layers that feed into classic ANN\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Initializing the CNN\n",
    "# Calling this object a 'classifier' because that's its job\n",
    "classifier = Sequential()\n",
    "\n",
    "# Step 1 - Convolution\n",
    "# Apply a method 'add' on the object 'classifier'\n",
    "# Filter = Feature Detector = Feature Kernel\n",
    "# 'Conv2D' (Number of Filters, (Filter Row, Filter Column), input shape of inputs = (3 color channels, 64x64 -> 256x256 dimension of 2D array in each channel))\n",
    "# Start with 32 filters, work your way up to 64 -> 128 -> 256\n",
    "# 'input_shape' needs all picture inputs to be the same shape and format (2D array for B&W, 3D for Color images with each 2D array channel being Blue/Green/Red)\n",
    "# 'input_shape' parameter shape matters (3,64,64) vs (64,64,3)\n",
    "# 'Relu' Rectifier Activation Function used to get rid of -ve pixel values and increase non-linearity\n",
    "classifier.add(Conv2D(32, (3, 3), input_shape = (64, 64, 3), activation = 'relu'))\n",
    "\n",
    "# Step 2 - Pooling\n",
    "# Reduces the size of the Feature Map by half (eg. 5x5 turns into 3x3 or 8x8 turns into 4x4)\n",
    "# Preserves Spatial Structure and performance of model while reducing computation time\n",
    "# 'pool_size' at least needs to be 2x2 to preserve Spatial Structure information (context around individual pixels)\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adding a second convolution layer to improve performance\n",
    "# Only need 'input_shape' for Input Layer\n",
    "classifier.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Step 3 - Flattening\n",
    "# Take all the Pooled Feature Maps and put them into one huge single Vector that will input into a classic NN\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Step 4 - Full connection\n",
    "# Add some fully connected hidden layers (start with a number of Node between input and output layers)\n",
    "# [Input Nodes(huge) - Output Nodes (2: Cat or Dog)] / 2 = ~128?...\n",
    "# 'Activation' function makes sure relevant Nodes get a stronger vote or no vote\n",
    "classifier.add(Dense(units = 128, activation = 'relu'))\n",
    "# Add final Output Layer with binary options\n",
    "classifier.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "# Compiling the CNN\n",
    "# 'adam' Stochastic Gradient Descent optimizer\n",
    "# 'loss' function. Logarithmic loss for 2 categories use 'binary_crossentropy' and 'categorical_crossentropy' for more objects\n",
    "# 'metric' is the a performance metric\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "# Part 2 - Fitting the CNN to the images\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Create random transformation from Data to increase Dataset and prevent overfitting\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "# 'batch_size' is the number of images that go through the CNN every weight update cycle\n",
    "# Increase 'target_size' to improve model accuracy \n",
    "\n",
    "training_set = train_datagen.flow_from_directory('../Data/training_set',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary')\n",
    "\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('../Data/test_set',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'fit_generator'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Increase 'epochs' to boost model performance (takes longer)\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m classifier\u001b[38;5;241m.\u001b[39mfit_generator(training_set,\n\u001b[0;32m      4\u001b[0m                          steps_per_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8000\u001b[39m,\n\u001b[0;32m      5\u001b[0m                          epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m      6\u001b[0m                          validation_data \u001b[38;5;241m=\u001b[39m test_set,\n\u001b[0;32m      7\u001b[0m                          validation_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'fit_generator'"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "# Increase 'epochs' to boost model performance (takes longer)\n",
    "classifier.fit_generator(training_set,\n",
    "                         steps_per_epoch = 8000,\n",
    "                         epochs = 1,\n",
    "                         validation_data = test_set,\n",
    "                         validation_steps = 2000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 62, 62, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 29, 29, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               802944    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 813,217\n",
      "Trainable params: 813,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Save model to file\n",
    "# Architecture of the model, allowing to reuse trained models\n",
    "# Weights of the model\n",
    "# Training configuration (loss, optimizer)\n",
    "# State of the optimizer, allowing to resume training exactly where you left off\n",
    "classifier.save('../Data/saved_model/CNN_Cat_Dog_Model.h5')\n",
    "\n",
    "# Examine model\n",
    "classifier.summary()\n",
    "\n",
    "# Examine Weights\n",
    "classifier.weights\n",
    "\n",
    "# Examine Optimizer\n",
    "classifier.optimizer\n",
    "\n",
    "\n",
    "\n",
    "# Load saved Model\n",
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('../Data/saved_model/CNN_Cat_Dog_Model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model class indices are: {'cats': 0, 'dogs': 1}\n",
      "\n",
      "Prediction: dog\n"
     ]
    }
   ],
   "source": [
    "# Part 3 - Making new predictions\n",
    "\n",
    "# Place a new picture of a cat or dog in 'single_prediction' folder and see if your model works\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('../Data/single_prediction/cat_or_dog_1.jpg', target_size = (64, 64))\n",
    "# Add a 3rd Color dimension to match Model expectation\n",
    "test_image = image.img_to_array(test_image)\n",
    "# Add one more dimension to beginning of image array so 'Predict' function can receive it (corresponds to Batch, even if only one batch)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = classifier.predict(test_image)\n",
    "# We now need to pull up the mapping between 0/1 and cat/dog\n",
    "training_set.class_indices\n",
    "# Map is 2D so check the first row, first column value\n",
    "if result[0][0] == 1:\n",
    "    prediction = 'dog'\n",
    "else:\n",
    "    prediction = 'cat'\n",
    "# Print result\n",
    "\n",
    "print(\"The model class indices are:\", training_set.class_indices)\n",
    "\n",
    "print(\"\\nPrediction: \" + prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
