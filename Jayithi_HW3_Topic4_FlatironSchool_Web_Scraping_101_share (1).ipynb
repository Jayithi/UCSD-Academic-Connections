{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDKmrDSfq6L7"
      },
      "source": [
        "<img src=\"https://www.dropbox.com/s/fchpltm5rnwd5ce/Flatiron%20Logo%202Wordmark.png?raw=1\" width=100 >\n",
        "\n",
        "# Web Scraping 101\n",
        "- James M. Irving, Ph.D.\n",
        "- james.irving.phd@gmail.com\n",
        "- Repo: https://github.com/jirvingphd/my_data_science_notes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqLBiwwB6I2k"
      },
      "source": [
        "# Quick Google Colab Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn_zdpwZ6IfO"
      },
      "source": [
        "**Google Colab Quick - Notes**\n",
        " 1. Open the sidebar! (the little  ' > ' button on the top-left of the document pane.)\n",
        "    - Use `Table of Contents` to Jump between the 3 major sections.\n",
        "    - Mount your google drive via the `Files `\n",
        "    - Note: to make a section appear in the Table of Contents, create a NEW text cell for the *header only*. This will also let you collapse all of the cells in the section to reduce clutter.\n",
        "\n",
        "    \n",
        " 2. Google Colab already has most common python packages.\n",
        "    - You can pip install anything by prepending an exclamation point\n",
        "    ```python\n",
        "    !pip install bs_ds\n",
        "    !pip install fake_useragent\n",
        "    !pip install lxml\n",
        "    ```\n",
        "    \n",
        "3. Open a notebook from github or save to github using `File > Upload Notebook` and `File> Save a copy in github`, respectively\n",
        "\n",
        "4. Using GPUs/TPUs\n",
        "    - `Runtime > Change Runtime Type > Hardware Acceleration`\n",
        "\n",
        "5. Run-Before and Run-After\n",
        "    - Go to `Runtime` and select `Run before` to run all cells up to the currently active cell\n",
        "    - Go to `Runtime` and select `Run after` to run all cells that follow the currently active cell\n",
        "\n",
        "6. Cloud Files with Colab\n",
        "    - **Open .csv's stored in a github repo directly with Pandas**:\n",
        "        - Go to the repo on GitHub, click on the csv file, then click on `Download` or `Raw` which will then change to show you the raw text. Copy and paste the link in your address bar (should start with www.rawgithubusercontent).\n",
        "        - In your notebook, do `df=pd.read_csv(url)` to load in the data.\n",
        "    - **Google Drive: Open sidebar > Files> click Mount Drive**\n",
        "        - or use this function:\n",
        "        ```python\n",
        "        def mount_google_drive(force_remount=True):\n",
        "            from google.colab import drive\n",
        "            print('drive_filepath=\"drive/My Drive/\"')\n",
        "            return drive.mount('/content/drive', force_remount=force_remount)\n",
        "        ```\n",
        "        - Then access files by file path like usual.\n",
        "        \n",
        "    - Dropbox Files: (like images or csv)\n",
        "        - Copy and paste the share link.\n",
        "        - Change the end of the link from `dl=0`to `dl=1`\n",
        "        \n",
        "6B. Function To Turn Google Drive Share links into usable image links for html\n",
        "\n",
        "```python\n",
        "def make_gdrive_file_url(share_url_from_gdrive):\n",
        "    \"\"\"accepts gdrive share url with format 'https://drive.google.com/open?id=`\n",
        "    and returns a pandas-usable link with format ''https://drive.google.com/uc?export=download&id='\"\"\"\n",
        "    import re\n",
        "    file_id = re.compile(r'id=(.*)')\n",
        "    fid = file_id.findall(share_url_from_gdrive)\n",
        "    prepend_url = 'https://drive.google.com/uc?export=download&id='\n",
        "    output_url = prepend_url + fid[0]\n",
        "    return output_url\n",
        "\n",
        "test_link = \"https://drive.google.com/open?id=1eHbOq-2TqGx4d2jZXrUdwNnJY_aM_7rj\" # airline passenger .csv\n",
        "file_link = make_gdrive_file_url(test_link)\n",
        "file_link\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAuVqqH66xeI"
      },
      "source": [
        "# Web Scraping 101"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Wnkf6zBwjsF"
      },
      "source": [
        "**Table of Contents - Shallow**\n",
        "1. Notes on Using BeautifulSoup\n",
        "2. Walk-through example/code\n",
        "    - My personal functions and then a working code frame using them.\n",
        "3. Notes Section for\n",
        " - After this, make sure to check out [Brandon's Web Scraping 202](https://github.com/cyranothebard/flatironschool_datascience/blob/master/Web%20Scraping%20202.ipynb)\n",
        " - He goes into using alternating ip addresses and more complex framework for harvesting content\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Recommended packages/tools to use\n",
        "1. `fake_useragent`\n",
        "    - pip-installable module that conveniently supplies fake user agent information to use in your request headers.\n",
        "    - recommended by udemy course\n",
        "2. `lxml`\n",
        "    - popular pip installable html parser (recommended by Udemy course)\n",
        "    - using `'html.parser'` in requests.get() did not work for me, I had to install lxml\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gh6WTxiVYqkh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47917991-0146-4f67-91ac-c9fc1551ea26"
      },
      "source": [
        "!pip install bs_ds\n",
        "!pip install fake_useragent\n",
        "!pip install lxml"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bs_ds in /usr/local/lib/python3.10/dist-packages (0.3.12)\n",
            "Requirement already satisfied: Click in /usr/local/lib/python3.10/dist-packages (from bs_ds) (8.1.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bs_ds) (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from bs_ds) (2.0.3)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from bs_ds) (0.13.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bs_ds) (3.7.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from bs_ds) (1.3.2)\n",
            "Requirement already satisfied: pydotplus in /usr/local/lib/python3.10/dist-packages (from bs_ds) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from bs_ds) (1.13.1)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (from bs_ds) (2.1.0)\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.10/dist-packages (from bs_ds) (7.34.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from IPython->bs_ds) (71.0.4)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from IPython->bs_ds) (0.19.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from IPython->bs_ds) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from IPython->bs_ds) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from IPython->bs_ds) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from IPython->bs_ds) (3.0.47)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from IPython->bs_ds) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from IPython->bs_ds) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from IPython->bs_ds) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from IPython->bs_ds) (4.9.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bs_ds) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bs_ds) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bs_ds) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bs_ds) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bs_ds) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bs_ds) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bs_ds) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bs_ds) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->bs_ds) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->bs_ds) (2024.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->bs_ds) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->bs_ds) (3.5.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.10/dist-packages (from xgboost->bs_ds) (2.22.3)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->IPython->bs_ds) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->IPython->bs_ds) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->IPython->bs_ds) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->bs_ds) (1.16.0)\n",
            "Requirement already satisfied: fake_useragent in /usr/local/lib/python3.10/dist-packages (1.5.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (4.9.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaWIHHicwtAr"
      },
      "source": [
        "## Using python's `requests` module:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JpBMzWysPb0"
      },
      "source": [
        "\n",
        "-  Use `requests` library to initiate connections to a website.\n",
        "- Check the status code returned to determine if connection was successful (status code=200)\n",
        "\n",
        "```python\n",
        "import requests\n",
        "url = 'https://en.wikipedia.org/wiki/Stock_market'\n",
        "\n",
        "# Connect to the url using requests.get\n",
        "response = requests.get(url)\n",
        "response.status_code\n",
        "```\n",
        "\n",
        " ___\n",
        "| Status Code | Code Meaning\n",
        "| --------- | -------------|\n",
        "1xx |   Informational\n",
        "2xx|    Success\n",
        "3xx|     Redirection\n",
        "4xx|     Client Error\n",
        "5xx |    Server Error\n",
        "\n",
        "___\n",
        "- **Note: You can add a `timeout` to `requests.get()` to avoid indefinite waiting**\n",
        "    - Best in multiples of 3 (`timeout=3` or `6` , `9` ,etc.)\n",
        "\n",
        "```python\n",
        "# Add a timeout to prevent hanging\n",
        "response = requests.get(url, timeout=3)\n",
        "response.status_code\n",
        "\n",
        "```\n",
        "- **`response` is a dictionary with the contents printed below**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5Rn-Y0N0q3B",
        "outputId": "8e3541f6-5f56-42b9-9986-1a13e288af89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import requests\n",
        "\n",
        "# I'm setting the URL to the Wikipedia page about the stock market.\n",
        "url = 'https://en.wikipedia.org/wiki/Stock_market'\n",
        "\n",
        "# Making a GET request to the URL with a timeout of 3 seconds.\n",
        "response = requests.get(url, timeout=3)\n",
        "\n",
        "# Checking the status code of the response.\n",
        "print('Status code: ', response.status_code)\n",
        "\n",
        "# If the status code is 200, the connection was successful.\n",
        "if response.status_code == 200:\n",
        "    print('Connection successful.\\n\\n')\n",
        "else:\n",
        "    # If the status code is not 200, there was an error.\n",
        "    print('Error. Check status code table.\\n\\n')\n",
        "\n",
        "# Printing out the contents of the response's headers.\n",
        "print(f\"{'---'*20}\\n\\tContents of Response.headers:\\n{'---'*20}\")\n",
        "\n",
        "# Iterating through the response headers and printing each key-value pair.\n",
        "for k, v in response.headers.items():\n",
        "    print(f\"{k:{25}}: {v:{40}}\")  # Printing headers with formatted spacing.\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Status code:  200\n",
            "Connection successful.\n",
            "\n",
            "\n",
            "------------------------------------------------------------\n",
            "\tContents of Response.headers:\n",
            "------------------------------------------------------------\n",
            "date                     : Mon, 29 Jul 2024 12:00:58 GMT           \n",
            "server                   : mw-web.eqiad.main-67d688ffb5-ldxtr      \n",
            "x-content-type-options   : nosniff                                 \n",
            "content-language         : en                                      \n",
            "origin-trial             : AonOP4SwCrqpb0nhZbg554z9iJimP3DxUDB8V4yu9fyyepauGKD0NXqTknWi4gnuDfMG6hNb7TDUDTsl0mDw9gIAAABmeyJvcmlnaW4iOiJodHRwczovL3dpa2lwZWRpYS5vcmc6NDQzIiwiZmVhdHVyZSI6IlRvcExldmVsVHBjZCIsImV4cGlyeSI6MTczNTM0Mzk5OSwiaXNTdWJkb21haW4iOnRydWV9\n",
            "accept-ch                :                                         \n",
            "vary                     : Accept-Encoding,Cookie,Authorization    \n",
            "last-modified            : Mon, 29 Jul 2024 11:56:50 GMT           \n",
            "content-type             : text/html; charset=UTF-8                \n",
            "content-encoding         : gzip                                    \n",
            "age                      : 2501                                    \n",
            "x-cache                  : cp1100 miss, cp1100 hit/2               \n",
            "x-cache-status           : hit-front                               \n",
            "server-timing            : cache;desc=\"hit-front\", host;desc=\"cp1100\"\n",
            "strict-transport-security: max-age=106384710; includeSubDomains; preload\n",
            "report-to                : { \"group\": \"wm_nel\", \"max_age\": 604800, \"endpoints\": [{ \"url\": \"https://intake-logging.wikimedia.org/v1/events?stream=w3c.reportingapi.network_error&schema_uri=/w3c/reportingapi/network_error/1.0.0\" }] }\n",
            "nel                      : { \"report_to\": \"wm_nel\", \"max_age\": 604800, \"failure_fraction\": 0.05, \"success_fraction\": 0.0}\n",
            "set-cookie               : WMF-Last-Access=29-Jul-2024;Path=/;HttpOnly;secure;Expires=Fri, 30 Aug 2024 12:00:00 GMT, WMF-Last-Access-Global=29-Jul-2024;Path=/;Domain=.wikipedia.org;HttpOnly;secure;Expires=Fri, 30 Aug 2024 12:00:00 GMT, WMF-DP=71e;Path=/;HttpOnly;secure;Expires=Tue, 30 Jul 2024 00:00:00 GMT, GeoIP=US:SC:North_Charleston:32.86:-79.97:v4; Path=/; secure; Domain=.wikipedia.org, NetworkProbeLimit=0.001;Path=/;Secure;SameSite=Lax;Max-Age=3600\n",
            "x-client-ip              : 35.196.92.24                            \n",
            "cache-control            : private, s-maxage=0, max-age=0, must-revalidate\n",
            "accept-ranges            : bytes                                   \n",
            "content-length           : 78170                                   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbpLnJrzDetE",
        "outputId": "508b8d2a-8c3c-4bd3-e43c-d37086fa4216",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Iterating through the response headers and printing each key-value pair.\n",
        "# Note: Adding :{number} inside the format string can help control the width of the printed values.\n",
        "for k, v in response.headers.items():\n",
        "    print(f\"{k:{25}}: {v:{40}}\")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "date                     : Mon, 29 Jul 2024 12:00:58 GMT           \n",
            "server                   : mw-web.eqiad.main-67d688ffb5-ldxtr      \n",
            "x-content-type-options   : nosniff                                 \n",
            "content-language         : en                                      \n",
            "origin-trial             : AonOP4SwCrqpb0nhZbg554z9iJimP3DxUDB8V4yu9fyyepauGKD0NXqTknWi4gnuDfMG6hNb7TDUDTsl0mDw9gIAAABmeyJvcmlnaW4iOiJodHRwczovL3dpa2lwZWRpYS5vcmc6NDQzIiwiZmVhdHVyZSI6IlRvcExldmVsVHBjZCIsImV4cGlyeSI6MTczNTM0Mzk5OSwiaXNTdWJkb21haW4iOnRydWV9\n",
            "accept-ch                :                                         \n",
            "vary                     : Accept-Encoding,Cookie,Authorization    \n",
            "last-modified            : Mon, 29 Jul 2024 11:56:50 GMT           \n",
            "content-type             : text/html; charset=UTF-8                \n",
            "content-encoding         : gzip                                    \n",
            "age                      : 2501                                    \n",
            "x-cache                  : cp1100 miss, cp1100 hit/2               \n",
            "x-cache-status           : hit-front                               \n",
            "server-timing            : cache;desc=\"hit-front\", host;desc=\"cp1100\"\n",
            "strict-transport-security: max-age=106384710; includeSubDomains; preload\n",
            "report-to                : { \"group\": \"wm_nel\", \"max_age\": 604800, \"endpoints\": [{ \"url\": \"https://intake-logging.wikimedia.org/v1/events?stream=w3c.reportingapi.network_error&schema_uri=/w3c/reportingapi/network_error/1.0.0\" }] }\n",
            "nel                      : { \"report_to\": \"wm_nel\", \"max_age\": 604800, \"failure_fraction\": 0.05, \"success_fraction\": 0.0}\n",
            "set-cookie               : WMF-Last-Access=29-Jul-2024;Path=/;HttpOnly;secure;Expires=Fri, 30 Aug 2024 12:00:00 GMT, WMF-Last-Access-Global=29-Jul-2024;Path=/;Domain=.wikipedia.org;HttpOnly;secure;Expires=Fri, 30 Aug 2024 12:00:00 GMT, WMF-DP=71e;Path=/;HttpOnly;secure;Expires=Tue, 30 Jul 2024 00:00:00 GMT, GeoIP=US:SC:North_Charleston:32.86:-79.97:v4; Path=/; secure; Domain=.wikipedia.org, NetworkProbeLimit=0.001;Path=/;Secure;SameSite=Lax;Max-Age=3600\n",
            "x-client-ip              : 35.196.92.24                            \n",
            "cache-control            : private, s-maxage=0, max-age=0, must-revalidate\n",
            "accept-ranges            : bytes                                   \n",
            "content-length           : 78170                                   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAPbGwxTDZbG"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjXVI4Qpw2_n"
      },
      "source": [
        "## Random Tips - Text Printing/Formatting:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkwN3WP80tiS"
      },
      "source": [
        "\n",
        "- **You can repeat strings by using multiplication**\n",
        "    - `'---'*20` will repeat the dashed lines 20 times\n",
        "\n",
        "- **You can determine how much space is alloted for a variable when using f-strings**\n",
        "    - Add a `:{##}` after the variable to specify the allocated width\n",
        "    - Add a `>` before the `{##}` to force alignment\n",
        "    - Add another symbol (like '.'' or '-') before `>` to add guiding-line/placeholder (like in a table of contents)\n",
        "\n",
        "```python\n",
        "print(f\"Status code: {response.status_code}\")\n",
        "print(f\"Status code: {response.status_code:>{20}}\")\n",
        "print(f\"Status code: {response.status_code:->{20}}\")\n",
        "```    \n",
        "```\n",
        "# Returns:\n",
        "Status code: 200\n",
        "Status code:                  200\n",
        "Status code: -----------------200\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hO3ac1hE8gr5"
      },
      "source": [
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSwUhVVTwvRq"
      },
      "source": [
        "## Quick Review -  HTML & Tags\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jZuk4QQ17nU"
      },
      "source": [
        "- All HTML pages have the following components\n",
        "    1. document declaration followed by html tag\n",
        "    \n",
        "    `<!DOCTYPE html>`<br>\n",
        "    `<html>`\n",
        "    2. Head\n",
        "     html tag<br>\n",
        "    `<head> <title></title></head>`\n",
        "    3. Body<br>\n",
        "    `<body>` ... content... `</body>`<br>\n",
        "    `</html>`\n",
        "\n",
        "- Html content is divdied into **tags** that specify the type of content.\n",
        "    - [Basic Tags Reference Table](https://www.w3schools.com/tags/ref_byfunc.asp)\n",
        "    - [Full Alphabetical Tag Reference Table](https://www.w3schools.com/tags/)\n",
        "    \n",
        "    - **tags** have attributes\n",
        "        - [Tag Attributes](https://www.w3schools.com/html/html_attributes.asp)\n",
        "        - Attributes are always defined in the start/opening tag.\n",
        "\n",
        "    - **tags** may have several content-creator-defined attributes such as `class` or `id`\n",
        "- We will **use the tag and its identifying attributes to isolate content** we want on a web page with BeautifulSoup.\n",
        "\n",
        "___\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7H_4da2vkGL"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "we-zGs8lw7kY"
      },
      "source": [
        "#  1) Using `BeautifulSoup`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDQT-D171lhn"
      },
      "source": [
        "\n",
        "## Cook a soup\n",
        "\n",
        "- Connect to a website using`response = requests.get(url)`\n",
        "- Feed `response.content` into BeautifulSoup\n",
        "- Must specify the parser that will analyze the contents\n",
        "    - default available is `'html.parser'`\n",
        "    - recommended is to install and use `lxml` [[lxml documentation](https://lxml.de/3.7/)]\n",
        "- use soup.prettify() to get a user-friendly version of the content to print\n",
        "\n",
        "```python\n",
        "# Define Url and establish connection\n",
        "url = 'https://en.wikipedia.org/wiki/Stock_market'\n",
        "response = requests.get(url, timeout=3)\n",
        "\n",
        "# Feed the response's .content into BeauitfulSoup\n",
        "page_content = response.content\n",
        "soup = BeautifulSoup(page_content,'lxml') #'html.parser')\n",
        "\n",
        "# Preview soup contents using .prettify()\n",
        "print(soup.prettify()[:2000])\n",
        "\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FieLZ63VVEXi"
      },
      "source": [
        "## What's in a Soup?\n",
        "- **A soup is essentially a collection of `tag objects`**\n",
        "    - each tag from the html is a tag object in the soup\n",
        "    - the tag's maintain the hierarchy of the html page, so tag objects will contain _other_ tag objects that were under it in the html tree.\n",
        "\n",
        "- **Each tag has a:**\n",
        "    - `.name`\n",
        "    - `.contents`\n",
        "    - `.string`\n",
        "    \n",
        "- **A tag can be access by name (like a column in a dataframe using dot notation)**\n",
        "    - and then you can access the tags within the new tag-variable just like the first tag\n",
        "    ```python\n",
        "    # Access tags by name\n",
        "    meta = soup.meta\n",
        "    head = soup.head\n",
        "    body = soup.body\n",
        "    # and so on...\n",
        "    ```\n",
        "- [!] ***BUT this will only return the FIRST tag of that type, to access all occurances of a tag-type, we will need to navigate the html family tree***\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZCLylw9RkVL"
      },
      "source": [
        "\n",
        "## Navigating the HTML Family Tree: Children, siblings, and parents\n",
        "\n",
        "- **Each tag is located within a tree-hierarchy of parents, siblings, and children**\n",
        "    - The family-relation is based on the identation level of the tags.\n",
        "\n",
        "- **Methods/attributes for the location/related tags of a tag**\n",
        "    - `.parent`, `.parents`\n",
        "    - `.child`, `.children`\n",
        "    - `.descendents`\n",
        "    - `.next_sibling`, `.previous_sibling`\n",
        "\n",
        "- *Note: a newline character `\\n` is also considered a tag/sibling/child*\n",
        "\n",
        "#### Accessing Child Tags\n",
        "\n",
        "- To get to later occurances of a tag type (i.e. the 2nd `<p>` tag in a tree), we need to navigate through the parent tag's `children`\n",
        "    - To access an iterable list of a tag's children use `.children`\n",
        "        - But, this only returns its *direct children*  (one indentation level down)     \n",
        "        \n",
        "    ```python\n",
        "    # print direct children of the body tag\n",
        "    body = soup.body\n",
        "    for child in body.children:\n",
        "        # print child if its not empty\n",
        "        print(child if child is not None else ' ', '\\n\\n')  # '\\n\\n' for visual separation\n",
        "    ```\n",
        "- To access *all children* use `.descendents`\n",
        "    - Returns all chidren and children of children\n",
        "    ```python\n",
        "    for child in body.descendents:\n",
        "        # print all children/grandchildren, etc\n",
        "        print(child if child is not None else ' ','\\n\\n')  \n",
        "    ```\n",
        "    \n",
        "#### Accessing Parent tags\n",
        "\n",
        "- To access the parent of a tag use `.parent`\n",
        "```python\n",
        "title = soup.head.title\n",
        "print(title.parent.name)\n",
        "```\n",
        "\n",
        "- To get a list of _all parents_ use `.parents`\n",
        "```python\n",
        "title = soup.head.title\n",
        "for parent in title.parents:\n",
        "    print(parent.name)\n",
        "```\n",
        "\n",
        "#### Accessing Sibling tags\n",
        "- siblings are tags in the same tree indentation level\n",
        "- `.next_sibling`, `.previous_sibling`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXktQsDvWW0A"
      },
      "source": [
        "## Searching Through Soup\n",
        "\n",
        "\n",
        "### Finding the target tags to isolate\n",
        "Using example  from  [Wikipedia article](https://en.wikipedia.org/wiki/Stock_market)\n",
        "where we are trying to isolate the body of the article content.\n",
        "\n",
        "\n",
        "- **Examine the website using Chrome's inspect view.**\n",
        "\n",
        "    - Press F12 or right-click > inspect\n",
        "\n",
        "    - Use the mouse selector tool (top left button) to explore the web page content for your desired target\n",
        "        - the web page element will be highlighted on the page itself and its corresponding entry in the document tree.\n",
        "        - Note: click on the web page with the selector in order to keep it selected in the document tree\n",
        "\n",
        "    - Take note of any identifying attributes for the target tag (class, id, etc)\n",
        "<img src=\"https://drive.google.com/uc?export-download&id=1KifQ_ukuXFdnCh1Tz1rwzA_cWkB_45mf\" width=450>\n",
        "\n",
        "### Using BeautifulSoup's search functions\n",
        "Note: while the process below is a decent summary, there is more nuance to html/css tags than I personally have been able to digest.\n",
        "    - If something doesn't work as expected/explained, please verify in the documentation.\n",
        "        - [BeauitfulSoup documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#beautiful-soup-documentation)\n",
        "        - [docs for .find_all()](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find-all)\n",
        "    \n",
        "- **BeautifulSoup has methods for searching through descendent-tags**\n",
        "    - `.find`\n",
        "    - `.find_all`\n",
        "    \n",
        "- **Using `.find_all()`**\n",
        "    - Searches through all descendent tags and returns a result set (list of tag objects)\n",
        "```python\n",
        "# How to get results from .find_all()\n",
        "results = soup.find_all(name, attrs, recursive, string, limit,**kwargs) `\n",
        "```        \n",
        "    - `.find_all()` parameters:\n",
        "        - `name` _(type of tags to consider)_\n",
        "            - only consider tags with this name\n",
        "                - Ex: 'a',  'div', 'p' ,etc.\n",
        "        - `atrrs`_(css attributes that you are looking for in your target tag)_\n",
        "            - enter an attribute such as the class or id as a string\n",
        "\n",
        "                `attrs='mw-content-ltr'`\n",
        "            - if passing more than one attribute, must use a dictionary:\n",
        "\n",
        "            `attrs={'class':'mw-content-ltr', 'id':'mw-content-text'}`\n",
        "        - `recursive`_(Default=True)_\n",
        "            - search all children (`True`)\n",
        "            - search only  direct children(`False`)\n",
        "\n",
        "        - `string`\n",
        "            - search for text _inside_ of tags instead of the tags themselves\n",
        "            - can be regular expression\n",
        "        - `limit`\n",
        "            - How many results you want it to return\n",
        "\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJFr-RC7vzI9",
        "outputId": "8aa28533-2469-4538-e8bd-c913a810cc99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install fake_useragent\n",
        "!pip install lxml"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fake_useragent in /usr/local/lib/python3.10/dist-packages (1.5.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (4.9.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUuCKNCdbYdS"
      },
      "source": [
        "# 2) Walk-through example/code\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwhquyO3x9q7"
      },
      "source": [
        "    - James functions\n",
        "    - Functional code scraping wikipedia pages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVfjE8RhHZq-"
      },
      "source": [
        "## James' Functions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xBlb5NuyeAC"
      },
      "source": [
        "- `soup = cook_soup_from_url(url)`\n",
        "    - make a beautiful soup from url\n",
        "-`soup_links = get_all_links(soup)`\n",
        "    - get all links from soup and return as a list.\n",
        "    \n",
        "-  `absolute_links = make_absolute_links(url, soup_links) `\n",
        "    - use If `soup_links` are relative links that do not include the website domain and start with '../' instead of 'https://www... ').\n",
        "    - then can use the `absolute_links` to make new soups to continue searching for your desired content.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTXEdbjQ_7RD"
      },
      "source": [
        "def mount_google_drive(force_remount=True):\n",
        "    # Importing the drive module from Google Colab to mount Google Drive.\n",
        "    from google.colab import drive\n",
        "\n",
        "    # Printing the path where Google Drive will be mounted.\n",
        "    print('drive_filepath=\"drive/My Drive/\"')\n",
        "\n",
        "    # Mounting Google Drive to the specified path. The force_remount parameter allows\n",
        "    # remounting the drive if it is already mounted.\n",
        "    return drive.mount('/content/drive', force_remount=force_remount)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIolt_tWAQ-E"
      },
      "source": [
        "mount_google_drive()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYgkNr2mA63V"
      },
      "source": [
        "drive_filepath=\"drive/My Drive/\"\n",
        "# import os\n",
        "# os.listdir(drive_filepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwR64_fYv3Up"
      },
      "source": [
        "def cook_soup_from_url(url, parser='lxml', sleep_time=0):\n",
        "    \"\"\"Uses requests to retrieve a webpage and returns a BeautifulSoup object created with the lxml parser.\"\"\"\n",
        "\n",
        "    # Importing necessary libraries: requests for making HTTP requests, sleep for delaying execution,\n",
        "    # and BeautifulSoup from bs4 for parsing HTML content.\n",
        "    import requests\n",
        "    from time import sleep\n",
        "    from bs4 import BeautifulSoup\n",
        "\n",
        "    # Pause execution for a specified amount of time (useful for rate limiting or waiting for page load).\n",
        "    sleep(sleep_time)\n",
        "\n",
        "    # Making a GET request to the provided URL.\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Checking the status code of the request. If it's not 200 (OK), raise an exception with an error message.\n",
        "    if response.status_code != 200:\n",
        "        raise Exception(f'Error: Status_code != 200.\\nstatus_code={response.status_code}')\n",
        "\n",
        "    # Extracting the content from the response.\n",
        "    c = response.content\n",
        "\n",
        "    # Parsing the content with BeautifulSoup using the specified parser ('lxml' by default).\n",
        "    soup = BeautifulSoup(c, parser)\n",
        "\n",
        "    # Returning the BeautifulSoup object.\n",
        "    return soup\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RM3tQkqyuoe"
      },
      "source": [
        "def get_all_links(soup):  # Function to find all links inside the provided BeautifulSoup object\n",
        "    \"\"\"Finds all links inside of soup that have the attributes(attr_kwds), which will be used in soup.findAll(attrs=attr_kwds).\n",
        "    Returns a list of links.\n",
        "    tag_type = 'a' or 'href'\"\"\"\n",
        "\n",
        "    # Find all 'a' tags in the BeautifulSoup object with optional attributes (attr_kwds)\n",
        "    all_a_tags = soup.findAll('a', attrs=kwds)\n",
        "\n",
        "    # Initialize an empty list to store the links\n",
        "    link_list = []\n",
        "\n",
        "    # Iterate through all 'a' tags\n",
        "    for link in all_a_tags:\n",
        "        # Extract the 'href' attribute from each 'a' tag\n",
        "        test_link = link.get('href')\n",
        "\n",
        "        # Append the extracted link to the list\n",
        "        link_list.append(test_link)\n",
        "\n",
        "    # Return the list of links\n",
        "    return link_list\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNjG2XYFyPIH"
      },
      "source": [
        "def make_absolute_links(source_url, rel_link_list):\n",
        "    \"\"\"Accepts the source_url for the source page of the rel_link_list and uses urljoin to return a list of valid absolute links.\"\"\"\n",
        "\n",
        "    # Import necessary functions from urllib.parse for URL manipulation\n",
        "    from urllib.parse import urljoin\n",
        "\n",
        "    # Initialize an empty list to store absolute links\n",
        "    absolute_links = []\n",
        "\n",
        "    # Loop through each relative link in the list\n",
        "    for link in rel_link_list:\n",
        "\n",
        "        # Convert the relative link to an absolute link using the source_url as the base\n",
        "        abs_link = urljoin(source_url, link)\n",
        "\n",
        "        # Add the absolute link to the list\n",
        "        absolute_links.append(abs_link)\n",
        "\n",
        "    # Return the list of absolute links\n",
        "    return absolute_links\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zo1sJyNE6ZCb"
      },
      "source": [
        "def cook_batch_of_soups(link_list, sleep_time=1): #, user_fun = extract_target_text):\n",
        "    \"\"\"Accepts a list of links to extract and save in a list of dictionaries of soups\n",
        "    with their relative URL path as their key.\n",
        "    Set user_fun to None to just extract full soups without user_extract.\"\"\"\n",
        "\n",
        "    # I import the necessary libraries\n",
        "    from time import sleep\n",
        "    from urllib.parse import urlparse\n",
        "\n",
        "    # I create an empty list to store dictionaries of soups\n",
        "    batch_of_soups = []\n",
        "\n",
        "    # I loop through each link in the link list\n",
        "    for link in link_list:\n",
        "        soup_dict = {}\n",
        "\n",
        "        # I convert the URL path into a dictionary key/title\n",
        "        url_dict_key_path = urlparse(link).path\n",
        "        url_dict_key = url_dict_key_path.split('/')[-1]\n",
        "\n",
        "        # I store the URL and path in the dictionary\n",
        "        soup_dict['_url'] = link\n",
        "        soup_dict['path'] = url_dict_key\n",
        "\n",
        "        # I create a BeautifulSoup object from the current link\n",
        "        page_soup = cook_soup_from_url(link, sleep_time=sleep_time)\n",
        "        soup_dict['soup'] = page_soup\n",
        "\n",
        "        # Uncomment if I provide a user-specified extraction function\n",
        "        # if user_fun != None:\n",
        "        #     # I add the user-specified extraction function\n",
        "        #     user_output = user_fun(page_soup) # Can add inputs to function\n",
        "        #     soup_dict['user_extract'] = user_output\n",
        "\n",
        "        # I add the current page's soup dictionary to the list\n",
        "        batch_of_soups.append(soup_dict)\n",
        "\n",
        "    # I return the list of soups\n",
        "    return batch_of_soups\n",
        "\n",
        "def extract_target_text(soup_or_tag, tag_name='p', attrs_dict=None, join_text=True, save_files=False):\n",
        "    \"\"\"User-specified function to extract specific content during 'cook_batch_of_soups'.\"\"\"\n",
        "\n",
        "    # I find all tags matching the specified name and attributes\n",
        "    if attrs_dict == None:\n",
        "        found_tags = soup_or_tag.find_all(name=tag_name)\n",
        "    else:\n",
        "        found_tags = soup_or_tag.find_all(name=tag_name, attrs=attrs_dict)\n",
        "\n",
        "    # I extract text from the found tags\n",
        "    output = [tag.text for tag in found_tags if tag.text is not None]\n",
        "\n",
        "    # If join_text is True, I concatenate all extracted texts\n",
        "    if join_text:\n",
        "        output = ' '.join(output)\n",
        "\n",
        "    # If save_files is True, I save the extracted text to a file\n",
        "    if save_files:\n",
        "        text = output # Use this line to save text content\n",
        "        filename = f\"drive/My Drive/text_extract_{url_dict_key}.txt\"  # Modify path if needed\n",
        "        soup_dict['filename'] = filename\n",
        "        with open(filename, 'w+') as f:\n",
        "            f.write(text)\n",
        "        print(f'File successfully saved as {filename}')\n",
        "\n",
        "    # I return the extracted text\n",
        "    return output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ODwEjKH92T5"
      },
      "source": [
        "def pickled_soup(soups, save_location='./', pickle_name='exported_soups.pckl'):\n",
        "    import pickle\n",
        "    import sys\n",
        "\n",
        "    # I create the file path by combining the save location and the pickle name\n",
        "    filepath = save_location + pickle_name\n",
        "\n",
        "    # I open the file in write-binary mode\n",
        "    with open(filepath, 'wb') as f:\n",
        "        # I use pickle to dump the soups into the file\n",
        "        pickle.dump(soups, f)\n",
        "\n",
        "    # I print a success message with the file path\n",
        "    return print(f'Soup successfully pickled. Stored as {filepath}.')\n",
        "\n",
        "def load_leftovers(filepath):\n",
        "    import pickle\n",
        "\n",
        "    # I print a message indicating the file being opened\n",
        "    print(f'Opening leftovers: {filepath}')\n",
        "\n",
        "    # I open the file in read-binary mode\n",
        "    with open(filepath, 'rb') as f:\n",
        "        # I use pickle to load the soups from the file\n",
        "        leftover_soup = pickle.load(f)\n",
        "\n",
        "    # I return the loaded soups\n",
        "    return leftover_soup\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ng-HN_rRymjI"
      },
      "source": [
        "## Walkthrough - using James' functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Pg3VWWmypij"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse, urljoin\n",
        "\n",
        "from fake_useragent import UserAgent\n",
        "url = 'https://en.wikipedia.org/wiki/Stock_market'\n",
        "soup = cook_soup_from_url(url,sleep_time=1)\n",
        "\n",
        "\n",
        "## Get all links that match are interal wikipedia redirects [yes?]\n",
        "kwds = {'class':'mw-redirect'}\n",
        "links = get_all_links(soup)#,kwds)\n",
        "\n",
        "\n",
        "# preview first 5 links\n",
        "print(links[:5])\n",
        "\n",
        "\n",
        "# Turn relative links into absolute links\n",
        "abs_links = make_absolute_links(url,links)\n",
        "print(abs_links[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tCwXLiN7UCk"
      },
      "source": [
        "# Selecting only the first 5 links to test\n",
        "abs_links_for_soups = abs_links[:5]\n",
        "\n",
        "\n",
        "# Cooking a batch of soups from those chosen links\n",
        "batch_of_soups = cook_batch_of_soups(abs_links_for_soups, sleep_time=2)\n",
        "\n",
        "# batch_of_soups is a list as long as the input link_list\n",
        "print(f'# of input links: == # of soups in batch:\\n{len(abs_links_for_soups)} == {len(batch_of_soups)}\\n')\n",
        "\n",
        "# batch_of_soups is a list of soup-dictionaries\n",
        "soup_dict = batch_of_soups[0]\n",
        "print('Each soup_dict has ',soup_dict.keys())\n",
        "\n",
        "# the page's soup is stored under soup_dict['soup']\n",
        "soup_from_soup_dict = soup_dict['soup']\n",
        "type(soup_from_soup_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFVnj20YmQK2"
      },
      "source": [
        "#### Notes on extracting content.\n",
        "- Edit the `extract_target_text function` in the James' functions settings or uncomment and use the `extract_target_text_custom function` below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwU8aPHJhAVm"
      },
      "source": [
        "## ADDING extract_target_text to precisely target text\n",
        "# def extract_target_text_custom(soup_or_tag,tag_name='p', attrs_dict=None, join_text =True, save_files=False):\n",
        "#     \"\"\"User-specified function to add extraction of specific content during 'cook batch of soups'\"\"\"\n",
        "\n",
        "#     if attrs_dict==None:\n",
        "#         found_tags = soup_or_tag.find_all(name=tag_name)\n",
        "#     else:\n",
        "#         found_tags = soup_or_tag.find_all(name=tag_name,attrs=attrs_dict)\n",
        "\n",
        "\n",
        "#     # if extracting from multiple tags\n",
        "#     output=[]\n",
        "#     output = [tag.text for tag in found_tags if tag.text is not None]\n",
        "\n",
        "#     if join_text == True:\n",
        "#         output = ' '.join(output)\n",
        "\n",
        "#     ## ADDING SAVING EACH\n",
        "#     if save_files==True:\n",
        "#         text = output #soup.body.string\n",
        "#         filename =f\"drive/My Drive/text_extract_{url_dict_key}.txt\"\n",
        "#         soup_dict['filename'] = filename\n",
        "#         with open(filename,'w+') as f:\n",
        "#             f.write(text)\n",
        "#         print(f'File  successfully saved as {filename}')\n",
        "\n",
        "#     return  output\n",
        "\n",
        "# ####################\n",
        "\n",
        "## RUN A LOOP TO ADD EXTRACTED TEXT TO EACH SOUP IN THE BATCH\n",
        "for i, soup_dict in enumerate(batch_of_soups):\n",
        "\n",
        "    # Get the soup from the dict\n",
        "    soup = soup_dict['soup']\n",
        "\n",
        "    # Extract text\n",
        "    extracted_text = extract_target_text(soup)\n",
        "\n",
        "    # Add key:value for results of extract\n",
        "    soup_dict['extracted'] = extracted_text\n",
        "\n",
        "    # Replace the old soup_dict with the new one with 'extracted'\n",
        "    batch_of_soups[i] = soup_dict\n",
        "\n",
        "example_extracted_text=batch_of_soups[0]['extracted']\n",
        "print(example_extracted_text[:1000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2lgbOyKyg0-"
      },
      "source": [
        "___\n",
        "___\n",
        "\n",
        "# Walk-through from Study Group (06/24/19):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUJ50zdSvjyL"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "from fake_useragent import UserAgent\n",
        "ua = UserAgent()\n",
        "\n",
        "header = {'user-agent':ua.chrome}\n",
        "print('Header:\\n',header)\n",
        "\n",
        "url ='https://en.wikipedia.org/wiki/Stock_market'\n",
        "response = requests.get(url, timeout=3, headers=header)\n",
        "\n",
        "print('Status code: ',response.status_code)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJwSP_bWnBL3"
      },
      "source": [
        "#### Example For Kate's Website\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "texdGvNkOoY8"
      },
      "source": [
        "# url='http://www.temis.nl/uvradiation/archives/v2.0/overpass/uv_Bern_Switzerland.dat'\n",
        "# batch_soups_kate = cook_batch_of_soups(url)\n",
        "# #THE URL IS NOT WORKING AND I DON'T KNOW WHAT URL TO USE.\n",
        "# ## Saving each page's body as a text file\n",
        "# for soup_dict in batch_soups_kate:\n",
        "#     text = soup_dict['soup'].body.string\n",
        "#     filename =f\"drive/My Drive/test_text_saving {soup_dict['url_dict_key']}.txt\"\n",
        "#     with open(filename,'w+') as f:\n",
        "#         f.write(text)\n",
        "\n",
        "# ## Loading in a file to test if working.\n",
        "# test_file = batch_soups_kate[0].filename\n",
        "# with open(filename,'r') as f:\n",
        "#     data = f.read()\n",
        "# print(data)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}