{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 2 : Topic 1: Online Tutorials for Deeper Understanding-Jayithi Gavva"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailed explanation of each topic:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.)Keras Tutorial: The Ultimate Beginner’s Guide to Deep Learning in Python**\n",
    "\n",
    "*Many people hear about deep learning and neural networks, even if they have no programming experience. These terms are often mentioned frequently. I wanted to learn about these topics in detail, and Keras is a library that simplifies the process of creating neural networks. By learning Keras now, I can build a strong foundation that will be helpful for me in my future tasks.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Things I Have Learned:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: Set Up Your Environment\n",
    "\n",
    "Setting up your environment ensures that all necessary libraries and tools are installed and correctly configured. Using Anaconda helps manage dependencies and environments efficiently. We are checking if the Python version and the versions of NumPy and Matplotlib to ensure that they are installed correctly.\n",
    "\n",
    "**Code**:\n",
    "```python\n",
    "# Verify Python version\n",
    "!python --version\n",
    "\n",
    "# Verify NumPy and Matplotlib installation\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "print(\"NumPy version:\", np.__version__)\n",
    "print(\"Matplotlib version:\", matplotlib.__version__)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Install Keras and TensorFlow\n",
    "\n",
    "**Explanation**:\n",
    "Keras and TensorFlow are crucial for building and training deep learning models. TensorFlow provides the backend for Keras operations.\n",
    "\n",
    "**Code**:\n",
    "```python\n",
    "# Install Keras and TensorFlow if not already installed\n",
    "!pip install keras tensorflow\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Import Libraries and Modules\n",
    "\n",
    "**Explanation**:\n",
    "Importing libraries is essential for utilizing their functions and classes. Setting a random seed ensures that results are reproducible. Here, we are importing the necessary libraries and setting a random seed for reproducibility. `Sequential` is used to build the model, while `Conv2D` and `MaxPooling2D` are for CNN layers.\n",
    "\n",
    "**Code**:\n",
    "```python\n",
    "import numpy as np\n",
    "np.random.seed(123)  # for reproducibility\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import mnist\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "The usage of libraries and modules:\n",
    "\n",
    "### 1. `numpy` (imported as `np`)\n",
    "- **Description**: NumPy is a fundamental package for scientific computing in Python. It provides support for arrays, matrices, and a large number of mathematical functions to operate on these data structures.\n",
    "- **Usage in Code**: \n",
    "  ```python\n",
    "  import numpy as np\n",
    "  ```\n",
    "  - `np.random.seed(123)`: Sets the seed for NumPy's random number generator for reproducibility. This ensures that the random numbers generated are the same every time the code is run, which is important for debugging and consistency.\n",
    "\n",
    "### 2. `matplotlib.pyplot` (imported as `plt`)\n",
    "- **Description**: Matplotlib is a plotting library for creating static, animated, and interactive visualizations in Python. `pyplot` is a module in Matplotlib that provides a MATLAB-like interface.\n",
    "- **Usage in Code**:\n",
    "  ```python\n",
    "  import matplotlib.pyplot as plt\n",
    "  ```\n",
    "  - `plt` is used to create plots and visualizations, such as displaying images or graphs.\n",
    "\n",
    "### 3. `tensorflow.keras.models.Sequential`\n",
    "- **Description**: Keras, as part of TensorFlow, is a high-level neural networks API written in Python. `Sequential` is a linear stack of layers where you can easily add new layers one at a time.\n",
    "- **Usage in Code**:\n",
    "  ```python\n",
    "  from tensorflow.keras.models import Sequential\n",
    "  ```\n",
    "  - `Sequential`: Used to initialize a neural network model where layers are added sequentially.\n",
    "\n",
    "### 4. `tensorflow.keras.layers.Dense`\n",
    "- **Description**: Dense is a fully connected neural network layer. Each neuron receives input from all neurons of the previous layer.\n",
    "- **Usage in Code**:\n",
    "  ```python\n",
    "  from tensorflow.keras.layers import Dense\n",
    "  ```\n",
    "  - `Dense`: Adds a densely connected NN layer to the model.\n",
    "\n",
    "### 5. `tensorflow.keras.layers.Dropout`\n",
    "- **Description**: Dropout is a regularization technique where randomly selected neurons are ignored during training, which helps prevent overfitting.\n",
    "- **Usage in Code**:\n",
    "  ```python\n",
    "  from tensorflow.keras.layers import Dropout\n",
    "  ```\n",
    "  - `Dropout`: Adds a dropout layer to the model.\n",
    "\n",
    "### 6. `tensorflow.keras.layers.Flatten`\n",
    "- **Description**: Flatten converts a multi-dimensional input into a single-dimensional vector, usually used at the transition from convolutional layers to fully connected layers.\n",
    "- **Usage in Code**:\n",
    "  ```python\n",
    "  from tensorflow.keras.layers import Flatten\n",
    "  ```\n",
    "  - `Flatten`: Flattens the input to make it suitable for fully connected layers.\n",
    "\n",
    "### 7. `tensorflow.keras.layers.Conv2D`\n",
    "- **Description**: Conv2D is a 2D convolutional layer that applies convolutional filters to 2D input data, commonly used in image processing.\n",
    "- **Usage in Code**:\n",
    "  ```python\n",
    "  from tensorflow.keras.layers import Conv2D\n",
    "  ```\n",
    "  - `Conv2D`: Adds a convolutional layer to the model.\n",
    "\n",
    "### 8. `tensorflow.keras.layers.MaxPooling2D`\n",
    "- **Description**: MaxPooling2D is a pooling layer that performs max pooling operation on spatial data, reducing the dimensionality and computational complexity.\n",
    "- **Usage in Code**:\n",
    "  ```python\n",
    "  from tensorflow.keras.layers import MaxPooling2D\n",
    "  ```\n",
    "  - `MaxPooling2D`: Adds a max pooling layer to the model.\n",
    "\n",
    "### 9. `tensorflow.keras.utils.to_categorical`\n",
    "- **Description**: This module provides utility functions, including functions to convert class vectors to binary class matrices (one-hot encoding).\n",
    "- **Usage in Code**:\n",
    "  ```python\n",
    "  from tensorflow.keras.utils import to_categorical\n",
    "  ```\n",
    "  - `to_categorical`: Used for utility functions like converting labels to one-hot encoding.\n",
    "\n",
    "### 10. `tensorflow.keras.datasets.mnist`\n",
    "- **Description**: The `mnist` dataset module provides access to the MNIST dataset, which is a large database of handwritten digits commonly used for training various image processing systems.\n",
    "- **Usage in Code**:\n",
    "  ```python\n",
    "  from tensorflow.keras.datasets import mnist\n",
    "  ```\n",
    "  - `mnist`: Provides functions to load the MNIST dataset for training and testing models.\n",
    "\n",
    "Here, the code sets up the libraries and modules needed to build and train a Convolutional Neural Network (CNN) on the MNIST dataset, which consists of 28x28 pixel grayscale images of handwritten digits.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Load Image Data from MNIST\n",
    "\n",
    "**Explanation**:\n",
    "Here we are loading the MNIST dataset, checking its shape, and visualizing a sample image to get an idea of the data. The MNIST dataset is a classic dataset for image classification tasks. Loading and visualizing the data is the first step in understanding the dataset.\n",
    "\n",
    "**Code**:\n",
    "```python\n",
    "# Load MNIST data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Check data shapes\n",
    "print(\"Training data shape:\", X_train.shape)\n",
    "print(\"Test data shape:\", X_test.shape)\n",
    "\n",
    "# Visualize a sample image\n",
    "plt.imshow(X_train[0], cmap='gray')\n",
    "plt.title(f\"Label: {y_train[0]}\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Preprocess Input Data\n",
    "\n",
    "**Explanation**:\n",
    "Preprocessing includes reshaping and normalizing the data to fit the model requirements and improve training efficiency.(specifically to include the channel dimension and normalize pixel values to a range of [0, 1].)\n",
    "\n",
    "**Code**:\n",
    "```python\n",
    "# Reshape data\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
    "\n",
    "# Normalize data\n",
    "X_train = X_train.astype('float32') / 255\n",
    "X_test = X_test.astype('float32') / 255\n",
    "\n",
    "print(\"Training data shape after reshaping:\", X_train.shape)\n",
    "print(\"Test data shape after reshaping:\", X_test.shape)\n",
    "```\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Preprocess Class Labels\n",
    "\n",
    "**Explanation**:\n",
    "Class labels need to be converted to a format suitable for classification (one-hot encoding) since the output of the network will be probabilities for each class.\n",
    "\n",
    "**Code**:\n",
    "```python\n",
    "# Convert class labels to one-hot encoding\n",
    "Y_train = to_categorical(y_train, 10)\n",
    "Y_test = to_categorical(y_test, 10)\n",
    "\n",
    "print(\"One-hot encoded training labels shape:\", Y_train.shape)\n",
    "print(\"One-hot encoded test labels shape:\", Y_test.shape)\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7: Define Model Architecture\n",
    "\n",
    "**Explanation**:\n",
    "Here we are building the model by stacking convolutional layers followed by pooling and dropout for regularization. We are also flattening the output and adding dense layers for classification.\n",
    "\n",
    "**Code**:\n",
    "```python\n",
    "model = Sequential()\n",
    "\n",
    "# Add Convolutional layers\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Flatten and Dense layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()  # Display the model architecture\n",
    "```\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Functions and their Definitions:\n",
    "\n",
    "1. **`Sequential()`**:\n",
    "   - Initializes a linear stack of layers for a neural network model.\n",
    "\n",
    "2. **`Conv2D(filters, kernel_size, activation, input_shape)`**:\n",
    "   - Adds a 2D convolutional layer with specified filters, kernel size, activation function, and input shape.\n",
    "\n",
    "3. **`MaxPooling2D(pool_size)`**:\n",
    "   - Adds a max pooling layer with a specified pool size to reduce dimensionality.\n",
    "\n",
    "4. **`Dropout(rate)`**:\n",
    "   - Adds a dropout layer with a specified dropout rate to prevent overfitting.\n",
    "\n",
    "5. **`Flatten()`**:\n",
    "   - Flattens the input to convert multi-dimensional data into a 1D vector.\n",
    "\n",
    "6. **`Dense(units, activation)`**:\n",
    "   - Adds a fully connected layer with a specified number of units and activation function.\n",
    "\n",
    "7. **`model.summary()`**:\n",
    "   - Displays a summary of the model architecture, including layers, output shapes, and the number of parameters.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 8: Compile Model\n",
    "\n",
    "**Explanation**:\n",
    "Compile the model by specifying the loss function, optimizer, and metrics to monitor during training. Here we are choosing `categorical_crossentropy` as the loss function for multi-class classification, and `adam` as the optimizer for efficient training.\n",
    "\n",
    "\n",
    "**Code**:\n",
    "```python\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Loss Function, Optimizer, and Metrics\n",
    "\n",
    "#### Loss Function\n",
    "- **Definition**: A loss function measures how well the model's predictions match the actual data. During training, the model tries to minimize this loss.\n",
    "- **`categorical_crossentropy`**: This loss function is used for multi-class classification problems. It calculates the difference between the predicted probability distribution and the true distribution. The goal is to minimize this difference, improving the accuracy of the model's predictions.\n",
    "\n",
    "#### Optimizer\n",
    "- **Definition**: An optimizer adjusts the weights of the network to minimize the loss function. It dictates how the model learns and updates itself.\n",
    "- **`adam`**: Adam (short for Adaptive Moment Estimation) is an optimization algorithm that adapts the learning rate for each parameter, making it well-suited for a wide range of problems. Adam is efficient, requires little memory, and is often considered a good default choice for most tasks.\n",
    "\n",
    "#### Metrics\n",
    "- **Definition**: Metrics are used to evaluate the performance of the model during training and testing. They are not used to train the model but to provide insight into its performance.\n",
    "- **`accuracy`**: This metric calculates the proportion of correctly predicted instances out of the total instances. For classification problems, it's measure of the model's performance. In the context of this code, it monitors how often predictions match the labels during training and evaluation.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 9: Fit Model on Training Data\n",
    "\n",
    "**Explanation**:\n",
    "Here we are training the model using the training data by specifying the batch size and number of epochs and we are also monitoring performance on a validation split to check for overfitting.\n",
    "\n",
    "**Code**:\n",
    "```python\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=32,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.2)  # Use a portion of data for validation\n",
    "```\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 10: Evaluate Model on Test Data\n",
    "\n",
    "**Explanation**:\n",
    "Evaluate the trained model on unseen test data to determine its generalization capability by assessing its loss and accuracy.\n",
    "\n",
    "**Code**:\n",
    "```python\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(f\"Test loss: {score[0]}\")\n",
    "print(f\"Test accuracy: {score[1]}\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THE TOTAL CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tanish Reddy\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  34/1875\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 20ms/step - accuracy: 0.3218 - loss: 1.9593"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 51\u001b[0m\n\u001b[0;32m     46\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     47\u001b[0m               optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     48\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# 9. Fit model on training data\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train, Y_train, \n\u001b[0;32m     52\u001b[0m           batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# 10. Evaluate model on test data\u001b[39;00m\n\u001b[0;32m     55\u001b[0m score \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test, Y_test, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Tanish Reddy\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Tanish Reddy\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:314\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[0;32m    313\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 314\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m    315\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[0;32m    316\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n",
      "File \u001b[1;32mc:\\Users\\Tanish Reddy\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Tanish Reddy\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\Tanish Reddy\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    879\u001b[0m     args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config\n\u001b[0;32m    880\u001b[0m )\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Tanish Reddy\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[0;32m    141\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Tanish Reddy\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall_preflattened(args)\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\Tanish Reddy\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_flat(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\Tanish Reddy\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[0;32m    255\u001b[0m     )\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Tanish Reddy\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1552\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1550\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1552\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[0;32m   1553\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1554\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   1555\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[0;32m   1556\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[0;32m   1557\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1558\u001b[0m   )\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1560\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1561\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1562\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1566\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1567\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\Tanish Reddy\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 3. Import libraries and modules\n",
    "import numpy as np\n",
    "np.random.seed(123)  # for reproducibility\n",
    " \n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "# from keras.layers import Convolution2D, MaxPooling2D\n",
    "# from keras.utils import np_utils\n",
    "# from keras.datasets import mnist\n",
    "# There was an error importing, so I changed the library to tensorflow.keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import mnist\n",
    " \n",
    "# 4. Load pre-shuffled MNIST data into train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    " \n",
    "# 5. Preprocess input data\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    " \n",
    "# 6. Preprocess class labels\n",
    "Y_train = to_categorical(y_train, 10)\n",
    "Y_test = to_categorical(y_test, 10)\n",
    " \n",
    "# 7. Define model architecture\n",
    "model = Sequential()\n",
    " \n",
    "model.add(Convolution2D(32, (3,3), activation='relu', input_shape=(28,28,1)))\n",
    "model.add(Convolution2D(32, (3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    " \n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    " \n",
    "# 8. Compile model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    " \n",
    "# 9. Fit model on training data\n",
    "model.fit(X_train, Y_train, \n",
    "          batch_size=32, epochs=10, verbose=1)\n",
    " \n",
    "# 10. Evaluate model on test data\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
